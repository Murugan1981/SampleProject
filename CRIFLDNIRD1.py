# script2_recreate_crif_ird.py
"""
Script 2: Re-create CRIF_LDN_IRD_YYYYMMDD.csv as per mapping.

Folder (as requested): H:\LDNM1\SIMM\Final

What this script does:
1) Loads:
   - MHBK_Sensitivities_YYYYMMDD.txt
   - MHBK_MX_PV_YYYYMMDD.csv
   - CSA_COUNTERPARTY_INFO_ALL_YYYYMMDD.csv
2) Filters SIMM sensitivities to IRD-only:
   - MUREXPRODUCTFAMILY == "IRD"
   - Sensitivity in {Risk_IRVol, Risk_Inflation, Risk_FX, Risk_IRCurve, Risk_XCcyBasis}
3) Builds CRIF_LDN_IRD output columns (with required blanks)
4) Enriches:
   - CP_ID from CSA file
   - PV from PV file
   - TRADE_DATE, END_DATE, C_CIF from CDW services (with caching)
5) Writes:
   - CRIF_LDN_IRD_<DATE>_QA.csv  (generated by this script)
   - optional: Script2_errors_<DATE>.csv (rows with missing mandatory fields)
   - cdw_cache_<DATE>.json

IMPORTANT:
- Some join keys / column names can differ across feeds. Update the CONFIG section below if needed.
"""

import os
import re
import json
import csv
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Optional, Tuple, List

import pandas as pd
import requests
from dotenv import load_dotenv
from requests_ntlm import HttpNtlmAuth
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==========================================================
# CONFIG (EDIT ONLY THIS SECTION IF YOUR COLUMN NAMES DIFFER)
# ==========================================================
BASE_DIR = Path(r"H:\LDNM1\SIMM\Final")

# If you want to hardcode exact files, set them here; otherwise keep None to auto-pick latest.
SIMM_SENS_FILE = None  # e.g. BASE_DIR / "MHBK_Sensitivities_20251128.txt"
PV_FILE = None         # e.g. BASE_DIR / "MHBK_MX_PV_20251128.csv"
CSA_FILE = None        # e.g. BASE_DIR / "CSA_COUNTERPARTY_INFO_ALL_20251128.csv"

# CDW endpoints from your screenshots
CDW_BASE = "https://svc-sit6-cdw.uk.mizuho-sc.com/mhbk"
INTRADAY_TRADES_URL = CDW_BASE + "/fpml/intradayTrades/{trade_id}"
LEGAL_ENTITY_CLIENTS_URL = CDW_BASE + "/common/legalEntityClients/{party_id}/"

# Expected SIMM columns (rename mapping if your file uses different names)
SIMM_COL_SENSITIVITY = "Sensitivity"
SIMM_COL_QUALIFIER = "Qualifier"
SIMM_COL_BUCKET = "Bucket"
SIMM_COL_LABEL1 = "Label1"
SIMM_COL_LABEL2 = "Label2"
SIMM_COL_VALUE = "Value"
SIMM_COL_CURRENCY = "Currency"
SIMM_COL_VALUEUSD = "ValueUSD"
SIMM_COL_PRODUCTFAMILY = "MUREXPRODUCTFAMILY"

# TRADE ID column in SIMM file:
# If your SIMM file does NOT contain TRADE_ID, you must add how you derive it (out of scope here).
SIMM_COL_TRADE_ID = "TRADE_ID"   # Change if your SIMM file uses a different column name

# CP join (CSA file -> CP_ID)
CSA_COL_CP_ID = "CP_ID"
# Choose the CSA join key column that matches your SIMM file's counterparty key.
# If you donâ€™t know, set this after inspecting CSA columns.
CSA_JOIN_KEY = "CP_ID"  # common fallback; update to the actual key column (e.g., "COUNTERPARTY_ID", "PARTY_ID", etc.)
SIMM_JOIN_KEY_FOR_CP = "CP_ID"  # column in SIMM that matches CSA_JOIN_KEY (update if needed)

# PV join: PV file key -> PV amount column
# Screenshots suggested MUREXROOTCONTRACTID is involved. Configure here.
PV_JOIN_KEY = "MUREXROOTCONTRACTID"
SIMM_JOIN_KEY_FOR_PV = "MUREXROOTCONTRACTID"
PV_VALUE_COL = "PV"  # update if PV file column name differs (e.g. "Amount", "PV_VALUE", etc.)

# Output formatting
DATE_FORMAT_OUTPUT = "%d/%m/%Y"  # as per layout DD/MM/YYYY
REQUEST_SLEEP_SECONDS = 0.05     # light throttling

ALLOWED_IRD_SENSITIVITIES = {
    "Risk_IRVol",
    "Risk_Inflation",
    "Risk_FX",
    "Risk_IRCurve",
    "Risk_XCcyBasis",
}

# Constants for NEW layout
CONST_PARTY_ID = "MIZBK"
CONST_IM_MODEL = "SIMM"
CONST_PRODUCT_CLASS = "RatesFX"

# Columns to output (NEW layout)
OUTPUT_COLUMNS = [
    "TRADE_ID",
    "PARTY_ID",
    "CP_ID",
    "IM_MODEL",
    "PRODUCT_CLASS",
    "TRADE_DATE",
    "END_DATE",
    "PRODUCT_TYPE",
    "NOTIONAL",
    "TRADE_CURRENCY",
    "NOTIONAL2",
    "TRADE_CURRENCY2",
    "VALUATION_DATE",
    "PV",
    "RISK_TYPE",
    "QUALIFIER",
    "BUCKET",
    "LABEL1",
    "LABEL2",
    "AMOUNT",
    "AMOUNT_CURRENCY",
    "AMOUNT_USD",
    "MASTER_CURRENCY",
    "MASTER_AMOUNT",
    "C_CIF",
]

# Mandatory non-blank columns in NEW layout (from your screenshot)
MANDATORY_NONBLANK = [
    "TRADE_ID", "PARTY_ID", "CP_ID", "IM_MODEL", "PRODUCT_CLASS", "TRADE_DATE", "END_DATE",
    "PV", "RISK_TYPE", "QUALIFIER", "BUCKET", "LABEL1", "LABEL2",
    "AMOUNT", "AMOUNT_CURRENCY", "AMOUNT_USD", "C_CIF"
]

# ==========================
# Helpers: file selection
# ==========================
def pick_latest_file(base: Path, regex_pattern: str) -> Path:
    candidates = [p for p in base.iterdir() if p.is_file() and re.fullmatch(regex_pattern, p.name, flags=re.IGNORECASE)]
    if not candidates:
        raise FileNotFoundError(f"No files matched: {regex_pattern} in {base}")

    def key_fn(p: Path):
        m = re.search(r"(\d{8})", p.name)
        date_part = int(m.group(1)) if m else 0
        return (date_part, p.stat().st_mtime)

    return sorted(candidates, key=key_fn, reverse=True)[0]

def extract_yyyymmdd_from_name(p: Path) -> str:
    m = re.search(r"(\d{8})", p.name)
    if not m:
        return datetime.now().strftime("%Y%m%d")
    return m.group(1)

# ==========================
# Helpers: delimiter sniffing
# ==========================
def sniff_delimiter(path: Path, fallback: str = ",") -> str:
    try:
        with path.open("r", encoding="utf-8", errors="ignore", newline="") as f:
            sample = f.read(8192)
        dialect = csv.Sniffer().sniff(sample, delimiters=[",", "|", "\t", ";"])
        return dialect.delimiter
    except Exception:
        return fallback

# ==========================
# Helpers: normalization
# ==========================
def norm_str(x) -> str:
    if x is None:
        return ""
    if isinstance(x, float) and pd.isna(x):
        return ""
    return str(x).strip()

def to_ddmmyyyy(date_str: str) -> str:
    s = norm_str(date_str)
    if not s:
        return ""
    dt = pd.to_datetime(s, errors="coerce", dayfirst=False)
    if pd.isna(dt):
        # try dayfirst
        dt = pd.to_datetime(s, errors="coerce", dayfirst=True)
    if pd.isna(dt):
        return s
    return dt.strftime(DATE_FORMAT_OUTPUT)

# ==========================
# Auth
# ==========================
def build_auth() -> HttpNtlmAuth:
    load_dotenv()
    user = os.getenv("USERNAME")
    pwd = os.getenv("PASSWORD")
    if not user or not pwd:
        raise RuntimeError("Missing USERNAME/PASSWORD in environment or .env file.")
    return HttpNtlmAuth(user, pwd)

# ==========================
# CDW parsing (regex-based)
# ==========================
def extract_first(xml_text: str, regex: str) -> Optional[str]:
    m = re.search(regex, xml_text, flags=re.IGNORECASE | re.DOTALL)
    return m.group(1).strip() if m else None

def parse_intraday_trade(xml_text: str) -> Dict[str, Optional[str]]:
    counterparty_block = extract_first(xml_text, r'<party\s+id="COUNTERPARTY".*?>.*?</party>')
    party_id = extract_first(counterparty_block, r"<partyId[^>]*>(.*?)</partyId>") if counterparty_block else None

    trade_date = extract_first(xml_text, r"<tradeDate>(.*?)</tradeDate>")

    termination_block = extract_first(xml_text, r"<terminationDate>.*?</terminationDate>")
    end_date = None
    if termination_block:
        end_date = extract_first(termination_block, r"<adjustedDate>(.*?)</adjustedDate>") or \
                   extract_first(termination_block, r"<unadjustedDate>(.*?)</unadjustedDate>")

    return {
        "counterparty_party_id": party_id,
        "trade_date": trade_date,
        "end_date": end_date
    }

def parse_legal_entity_client_ccif(xml_text: str) -> Optional[str]:
    return extract_first(xml_text, r'<identifier\s+name="MIZUHO_CCIF_NO"\s*>(.*?)</identifier>')

# ==========================
# CDW cache + fetch
# ==========================
class CDWCache:
    def __init__(self, path: Path):
        self.path = path
        self.data = {"intraday": {}, "ccif": {}}
        if path.exists():
            try:
                self.data = json.loads(path.read_text(encoding="utf-8"))
            except Exception:
                pass

    def save(self):
        self.path.write_text(json.dumps(self.data, indent=2), encoding="utf-8")

def http_get(auth: HttpNtlmAuth, url: str, timeout=30) -> str:
    r = requests.get(url, auth=auth, verify=False, timeout=timeout)
    r.raise_for_status()
    return r.text

def cdw_get_intraday(auth: HttpNtlmAuth, cache: CDWCache, trade_id: str) -> Dict[str, Optional[str]]:
    k = norm_str(trade_id)
    if not k:
        return {"counterparty_party_id": None, "trade_date": None, "end_date": None}
    if k in cache.data["intraday"]:
        return cache.data["intraday"][k]

    xml = http_get(auth, INTRADAY_TRADES_URL.format(trade_id=k))
    parsed = parse_intraday_trade(xml)
    cache.data["intraday"][k] = parsed
    time.sleep(REQUEST_SLEEP_SECONDS)
    return parsed

def cdw_get_ccif(auth: HttpNtlmAuth, cache: CDWCache, party_id: str) -> Optional[str]:
    k = norm_str(party_id)
    if not k:
        return None
    if k in cache.data["ccif"]:
        return cache.data["ccif"][k]

    xml = http_get(auth, LEGAL_ENTITY_CLIENTS_URL.format(party_id=k))
    ccif = parse_legal_entity_client_ccif(xml)
    cache.data["ccif"][k] = ccif
    time.sleep(REQUEST_SLEEP_SECONDS)
    return ccif

# ==========================
# Loading input files
# ==========================
def load_simm_file(path: Path) -> pd.DataFrame:
    delim = sniff_delimiter(path, fallback="|")  # SIMM .txt often pipe-delimited
    df = pd.read_csv(path, sep=delim, dtype=str, low_memory=False, encoding="utf-8", engine="python")
    df.columns = [c.strip() for c in df.columns]
    return df

def load_csv_file(path: Path) -> pd.DataFrame:
    delim = sniff_delimiter(path, fallback=",")
    df = pd.read_csv(path, sep=delim, dtype=str, low_memory=False, encoding="utf-8", engine="python")
    df.columns = [c.strip() for c in df.columns]
    return df

# ==========================
# Build CRIF IRD
# ==========================
def validate_required_columns(df: pd.DataFrame, required: List[str], file_label: str):
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise RuntimeError(f"{file_label} missing required columns: {missing}")

def recreate_crif_ird(
    simm: pd.DataFrame,
    pv_df: pd.DataFrame,
    csa_df: pd.DataFrame,
    auth: HttpNtlmAuth,
    cache: CDWCache
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Returns:
      output_df: CRIF_LDN_IRD dataframe
      errors_df: rows that fail mandatory nonblank checks (with reason)
    """

    # --- Validate essential SIMM columns ---
    validate_required_columns(simm, [
        SIMM_COL_SENSITIVITY, SIMM_COL_QUALIFIER, SIMM_COL_BUCKET,
        SIMM_COL_LABEL1, SIMM_COL_LABEL2, SIMM_COL_VALUE, SIMM_COL_CURRENCY, SIMM_COL_VALUEUSD,
        SIMM_COL_PRODUCTFAMILY, SIMM_COL_TRADE_ID
    ], "SIMM Sensitivities")

    # --- Filter IRD ---
    simm_f = simm.copy()
    simm_f[SIMM_COL_PRODUCTFAMILY] = simm_f[SIMM_COL_PRODUCTFAMILY].map(norm_str)
    simm_f[SIMM_COL_SENSITIVITY] = simm_f[SIMM_COL_SENSITIVITY].map(norm_str)

    simm_f = simm_f[
        (simm_f[SIMM_COL_PRODUCTFAMILY].str.upper() == "IRD") &
        (simm_f[SIMM_COL_SENSITIVITY].isin(ALLOWED_IRD_SENSITIVITIES))
    ].copy()

    # --- Base output mapping from SIMM ---
    out = pd.DataFrame()
    out["TRADE_ID"] = simm_f[SIMM_COL_TRADE_ID].map(norm_str)
    out["PARTY_ID"] = CONST_PARTY_ID
    out["CP_ID"] = ""  # to be joined
    out["IM_MODEL"] = CONST_IM_MODEL
    out["PRODUCT_CLASS"] = CONST_PRODUCT_CLASS

    # CDW enrich placeholders
    out["TRADE_DATE"] = ""
    out["END_DATE"] = ""

    # Blank-by-design fields
    out["PRODUCT_TYPE"] = ""
    out["NOTIONAL"] = ""
    out["TRADE_CURRENCY"] = ""
    out["NOTIONAL2"] = ""
    out["TRADE_CURRENCY2"] = ""
    out["VALUATION_DATE"] = ""

    # PV + risk columns
    out["PV"] = ""
    out["RISK_TYPE"] = simm_f[SIMM_COL_SENSITIVITY].map(norm_str)
    out["QUALIFIER"] = simm_f[SIMM_COL_QUALIFIER].map(norm_str)
    out["BUCKET"] = simm_f[SIMM_COL_BUCKET].map(norm_str)
    out["LABEL1"] = simm_f[SIMM_COL_LABEL1].map(norm_str)
    out["LABEL2"] = simm_f[SIMM_COL_LABEL2].map(norm_str)
    out["AMOUNT"] = simm_f[SIMM_COL_VALUE].map(norm_str)
    out["AMOUNT_CURRENCY"] = simm_f[SIMM_COL_CURRENCY].map(norm_str).str.upper()
    out["AMOUNT_USD"] = simm_f[SIMM_COL_VALUEUSD].map(norm_str)

    # Master fields blank
    out["MASTER_CURRENCY"] = ""
    out["MASTER_AMOUNT"] = ""

    out["C_CIF"] = ""  # to be CDW-derived

    # --- Join CP_ID from CSA file ---
    # NOTE: This join is configuration-driven. Update CSA_JOIN_KEY & SIMM_JOIN_KEY_FOR_CP as needed.
    if CSA_JOIN_KEY in csa_df.columns and SIMM_JOIN_KEY_FOR_CP in simm_f.columns and CSA_COL_CP_ID in csa_df.columns:
        csa_lu = csa_df[[CSA_JOIN_KEY, CSA_COL_CP_ID]].copy()
        csa_lu[CSA_JOIN_KEY] = csa_lu[CSA_JOIN_KEY].map(norm_str)
        csa_lu[CSA_COL_CP_ID] = csa_lu[CSA_COL_CP_ID].map(norm_str)
        csa_lu = csa_lu.drop_duplicates(subset=[CSA_JOIN_KEY])

        tmp_key = simm_f[SIMM_JOIN_KEY_FOR_CP].map(norm_str).reset_index(drop=True)
        out["_CP_JOIN_KEY_"] = tmp_key

        csa_map = dict(zip(csa_lu[CSA_JOIN_KEY], csa_lu[CSA_COL_CP_ID]))
        out["CP_ID"] = out["_CP_JOIN_KEY_"].map(lambda k: csa_map.get(k, ""))

        out = out.drop(columns=["_CP_JOIN_KEY_"], errors="ignore")
    else:
        # CP_ID is mandatory in NEW file; if join isn't configured correctly, we keep blanks and flag errors later.
        pass

    # --- Join PV from PV file ---
    # NOTE: Configure PV_JOIN_KEY, SIMM_JOIN_KEY_FOR_PV, PV_VALUE_COL properly for your actual files.
    if PV_JOIN_KEY in pv_df.columns and PV_VALUE_COL in pv_df.columns and SIMM_JOIN_KEY_FOR_PV in simm_f.columns:
        pv_lu = pv_df[[PV_JOIN_KEY, PV_VALUE_COL]].copy()
        pv_lu[PV_JOIN_KEY] = pv_lu[PV_JOIN_KEY].map(norm_str)
        pv_lu[PV_VALUE_COL] = pv_lu[PV_VALUE_COL].map(norm_str)
        pv_lu = pv_lu.drop_duplicates(subset=[PV_JOIN_KEY])

        pv_map = dict(zip(pv_lu[PV_JOIN_KEY], pv_lu[PV_VALUE_COL]))
        out["PV"] = simm_f[SIMM_JOIN_KEY_FOR_PV].map(norm_str).map(lambda k: pv_map.get(k, ""))
    else:
        # PV is mandatory in NEW file; will be flagged later if blank
        pass

    # --- CDW enrich TRADE_DATE, END_DATE, C_CIF (cached per trade) ---
    trade_ids = out["TRADE_ID"].dropna().map(norm_str)
    unique_trade_ids = [t for t in trade_ids.unique().tolist() if t]

    trade_to_dates: Dict[str, Tuple[str, str]] = {}
    trade_to_ccif: Dict[str, str] = {}

    for t in unique_trade_ids:
        try:
            intraday = cdw_get_intraday(auth, cache, t)
            td = to_ddmmyyyy(intraday.get("trade_date") or "")
            ed = to_ddmmyyyy(intraday.get("end_date") or "")
            party_id = intraday.get("counterparty_party_id")

            ccif = ""
            if party_id:
                ccif_val = cdw_get_ccif(auth, cache, party_id)
                ccif = norm_str(ccif_val)

            trade_to_dates[t] = (td, ed)
            trade_to_ccif[t] = ccif
        except Exception:
            trade_to_dates[t] = ("", "")
            trade_to_ccif[t] = ""

    out["TRADE_DATE"] = out["TRADE_ID"].map(lambda t: trade_to_dates.get(norm_str(t), ("", ""))[0])
    out["END_DATE"] = out["TRADE_ID"].map(lambda t: trade_to_dates.get(norm_str(t), ("", ""))[1])
    out["C_CIF"] = out["TRADE_ID"].map(lambda t: trade_to_ccif.get(norm_str(t), ""))

    # --- Ensure final column order ---
    for c in OUTPUT_COLUMNS:
        if c not in out.columns:
            out[c] = ""
    out = out[OUTPUT_COLUMNS].copy()

    # --- Mandatory nonblank checks ---
    errors = []
    for idx, row in out.iterrows():
        missing = [c for c in MANDATORY_NONBLANK if not norm_str(row.get(c, ""))]
        if missing:
            errors.append({
                "ROW_INDEX": idx,
                "TRADE_ID": norm_str(row.get("TRADE_ID", "")),
                "MISSING_COLUMNS": ", ".join(missing)
            })
    errors_df = pd.DataFrame(errors)

    return out, errors_df

# ==========================
# Main
# ==========================
def main():
    if not BASE_DIR.exists():
        raise FileNotFoundError(f"Folder not found: {BASE_DIR}")

    # Pick latest files if not specified
    simm_path = Path(SIMM_SENS_FILE) if SIMM_SENS_FILE else pick_latest_file(BASE_DIR, r"MHBK_Sensitivities_\d{8}\.txt")
    pv_path = Path(PV_FILE) if PV_FILE else pick_latest_file(BASE_DIR, r"MHBK_MX_PV_\d{8}\.csv")
    csa_path = Path(CSA_FILE) if CSA_FILE else pick_latest_file(BASE_DIR, r"CSA_COUNTERPARTY_INFO_ALL_\d{8}\.csv")

    yyyymmdd = extract_yyyymmdd_from_name(simm_path)

    out_file = BASE_DIR / f"CRIF_LDN_IRD_{yyyymmdd}_QA.csv"
    err_file = BASE_DIR / f"Script2_errors_{yyyymmdd}.csv"
    cache_file = BASE_DIR / f"cdw_cache_{yyyymmdd}.json"

    print("Using files:")
    print(f"  SIMM: {simm_path}")
    print(f"  PV  : {pv_path}")
    print(f"  CSA : {csa_path}")

    # Load inputs
    simm = load_simm_file(simm_path)
    pv_df = load_csv_file(pv_path)
    csa_df = load_csv_file(csa_path)

    # Build auth + cache
    auth = build_auth()
    cache = CDWCache(cache_file)

    # Recreate
    output_df, errors_df = recreate_crif_ird(simm, pv_df, csa_df, auth, cache)

    # Save cache
    cache.save()

    # Write output
    output_df.to_csv(out_file, index=False, encoding="utf-8")
    print(f"\nGenerated: {out_file}  (rows={len(output_df)})")

    # Write errors if any
    if len(errors_df) > 0:
        errors_df.to_csv(err_file, index=False, encoding="utf-8")
        print(f"Errors written: {err_file}  (rows={len(errors_df)})")
        print("NOTE: Missing mandatory fields usually indicates join-key config needs adjustment.")
    else:
        print("No mandatory-field errors detected.")

    print("\nDone.")

if __name__ == "__main__":
    main()
